{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e218614e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            Question  \\\n",
      "0                 first work generally recognized ai   \n",
      "1  source drawn formation first work generally re...   \n",
      "2                      created hebbian learning rule   \n",
      "3                         first neural network built   \n",
      "4                        first neural network called   \n",
      "\n",
      "                                              Answer  \n",
      "0        Warren McCulloch and Walter Pitts (1943).\\n  \n",
      "1  knowledge of the basic physiology and function...  \n",
      "2                              Donald Hebb (1949).\\n  \n",
      "3                                            1950.\\n  \n",
      "4                                       The SNARC.\\n  \n"
     ]
    }
   ],
   "source": [
    "#preprocess step\n",
    "\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "\n",
    "# DATA\n",
    "df = pd.read_csv(r\"C:\\Users\\nh013\\Desktop\\chatboot Q and A dataset\\AI.csv\")\n",
    "\n",
    "# DROP MISSING VALUES\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# REMOVE ANY SPESIAL CHARECTER AND CONVERT TEXT TO LOWER CASE\n",
    "\n",
    "df['Question'] = df['Question'].apply(lambda x: re.sub(r'[^\\w\\s]', '', x.lower()))\n",
    "\n",
    "# REMOVE STOP WORDS\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "df['Question'] = df['Question'].apply(lambda x: ' '.join([word for word in x.split() if word not in stop_words]))\n",
    "\n",
    "# LEMMATIZATION\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "df['Question'] = df['Question'].apply(lambda x: ' '.join([lemmatizer.lemmatize(word) for word in x.split()]))\n",
    "\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f903748f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6b38b746",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            Question  \\\n",
      "0                 first work generally recognized ai   \n",
      "1  source drawn formation first work generally re...   \n",
      "2                      created hebbian learning rule   \n",
      "3                         first neural network built   \n",
      "4                        first neural network called   \n",
      "\n",
      "                                              Answer  1963  1965  1969  1975  \\\n",
      "0        Warren McCulloch and Walter Pitts (1943).\\n   0.0   0.0   0.0   0.0   \n",
      "1  knowledge of the basic physiology and function...   0.0   0.0   0.0   0.0   \n",
      "2                              Donald Hebb (1949).\\n   0.0   0.0   0.0   0.0   \n",
      "3                                            1950.\\n   0.0   0.0   0.0   0.0   \n",
      "4                                       The SNARC.\\n   0.0   0.0   0.0   0.0   \n",
      "\n",
      "   1981  1988  1990s  1997  ...  within  word      work  world  worry  would  \\\n",
      "0   0.0   0.0    0.0   0.0  ...     0.0   0.0  0.438322    0.0    0.0    0.0   \n",
      "1   0.0   0.0    0.0   0.0  ...     0.0   0.0  0.319463    0.0    0.0    0.0   \n",
      "2   0.0   0.0    0.0   0.0  ...     0.0   0.0  0.000000    0.0    0.0    0.0   \n",
      "3   0.0   0.0    0.0   0.0  ...     0.0   0.0  0.000000    0.0    0.0    0.0   \n",
      "4   0.0   0.0    0.0   0.0  ...     0.0   0.0  0.000000    0.0    0.0    0.0   \n",
      "\n",
      "   wumpus  year  yielded  zerosum  \n",
      "0     0.0   0.0      0.0      0.0  \n",
      "1     0.0   0.0      0.0      0.0  \n",
      "2     0.0   0.0      0.0      0.0  \n",
      "3     0.0   0.0      0.0      0.0  \n",
      "4     0.0   0.0      0.0      0.0  \n",
      "\n",
      "[5 rows x 769 columns]\n"
     ]
    }
   ],
   "source": [
    "# USING TEXT VECTORIZATION\n",
    "# TF-IDF (Term Frequency-Inverse Document Frequency)\n",
    "\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# DATA\n",
    "df = pd.read_csv(r\"C:\\Users\\nh013\\Desktop\\chatboot Q and A dataset\\AI.csv\")\n",
    "\n",
    "# DROP MISSING VALUE\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# REMOVE ANY SPECIAL CHERECTER AND CONVERT LOWERCASE FORM\n",
    "df['Question'] = df['Question'].apply(lambda x: re.sub(r'[^\\w\\s]', '', x.lower()))\n",
    "\n",
    "# REMOVE STOPWORDS\n",
    "stop_words = set(stopwords.words('english'))\n",
    "df['Question'] = df['Question'].apply(lambda x: ' '.join([word for word in x.split() if word not in stop_words]))\n",
    "\n",
    "# LEMMATIZATION\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "df['Question'] = df['Question'].apply(lambda x: ' '.join([lemmatizer.lemmatize(word) for word in x.split()]))\n",
    "\n",
    "# PERFORM TEXT VECTORIZATION\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(df['Question'])\n",
    "\n",
    "# Convert the vectorized data to a DataFrame\n",
    "vectorized_df = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "# CONCATINATE THE VECTORIZED DATA WITH THE ORGINAL DATA FRAME\n",
    "df = pd.concat([df, vectorized_df], axis=1)\n",
    "\n",
    "\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19cbced9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "74bd0e39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            Question  \\\n",
      "0                 first work generally recognized ai   \n",
      "1  source drawn formation first work generally re...   \n",
      "2                      created lesbian learning rule   \n",
      "3                         first neural network built   \n",
      "4                        first neural network called   \n",
      "\n",
      "                                              Answer  \n",
      "0        Warren McCulloch and Walter Pitts (1943).\\n  \n",
      "1  knowledge of the basic physiology and function...  \n",
      "2                              Donald Hebb (1949).\\n  \n",
      "3                                            1950.\\n  \n",
      "4                                       The SNARC.\\n  \n"
     ]
    }
   ],
   "source": [
    "#To improve the quality of the input data in terms of handling typos, normalizing abbreviations, and addressing common \n",
    "#variations in user input\n",
    "#update preprocessing..........\n",
    "\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "from spellchecker import SpellChecker\n",
    "\n",
    "#DATA\n",
    "df = pd.read_csv(r\"C:\\Users\\nh013\\Desktop\\chatboot Q and A dataset\\AI.csv\")\n",
    "\n",
    "\n",
    "# DROP ROWS WITH MISSING VALUES\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# REMOVE ANY SPECIAL CHERECTER AND CONVERT LOWERCASE FORM\n",
    "df['Question'] = df['Question'].apply(lambda x: re.sub(r'[^\\w\\s]', '', x.lower()))\n",
    "\n",
    "# REMOVE STOPWORDS\n",
    "stop_words = set(stopwords.words('english'))\n",
    "df['Question'] = df['Question'].apply(lambda x: ' '.join([word for word in x.split() if word not in stop_words]))\n",
    "\n",
    "# LEMMATIZATION\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "df['Question'] = df['Question'].apply(lambda x: ' '.join([lemmatizer.lemmatize(word) for word in x.split()]))\n",
    "\n",
    "# SPELL-CHECKING\n",
    "spell = SpellChecker()\n",
    "df['Question'] = df['Question'].apply(lambda x: ' '.join([spell.correction(word) if spell.correction(word) is not None else word for word in x.split()]))\n",
    "\n",
    "# ABBREVIATION(Example: 'can't' to 'cannot')\n",
    "abbreviation_mapping = {\n",
    "    \"can't\": \"cannot\",\n",
    "    \"won't\": \"will not\",\n",
    "   \n",
    "}\n",
    "df['Question'] = df['Question'].apply(lambda x: ' '.join([abbreviation_mapping.get(word, word) for word in x.split()]))\n",
    "\n",
    "# REMOVE ANY ROWS WITH MISSING OR EMTY  VALUES AFTER PROCESSING\n",
    "df.dropna(subset=['Question'], inplace=True)\n",
    "\n",
    "\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2fa31a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c42c337",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            Question  \\\n",
      "0                 first work generally recognized ai   \n",
      "1  source drawn formation first work generally re...   \n",
      "2                      created lesbian learning rule   \n",
      "3                         first neural network built   \n",
      "4                        first neural network called   \n",
      "\n",
      "                                              Answer  1963  1965  1969  1975  \\\n",
      "0        Warren McCulloch and Walter Pitts (1943).\\n   0.0   0.0   0.0   0.0   \n",
      "1  knowledge of the basic physiology and function...   0.0   0.0   0.0   0.0   \n",
      "2                              Donald Hebb (1949).\\n   0.0   0.0   0.0   0.0   \n",
      "3                                            1950.\\n   0.0   0.0   0.0   0.0   \n",
      "4                                       The SNARC.\\n   0.0   0.0   0.0   0.0   \n",
      "\n",
      "   1981  1988  1990s  1997  ...  winter  within  word      work  world  worry  \\\n",
      "0   0.0   0.0    0.0   0.0  ...     0.0     0.0   0.0  0.438322    0.0    0.0   \n",
      "1   0.0   0.0    0.0   0.0  ...     0.0     0.0   0.0  0.319463    0.0    0.0   \n",
      "2   0.0   0.0    0.0   0.0  ...     0.0     0.0   0.0  0.000000    0.0    0.0   \n",
      "3   0.0   0.0    0.0   0.0  ...     0.0     0.0   0.0  0.000000    0.0    0.0   \n",
      "4   0.0   0.0    0.0   0.0  ...     0.0     0.0   0.0  0.000000    0.0    0.0   \n",
      "\n",
      "   would  year  yielded  zeros  \n",
      "0    0.0   0.0      0.0    0.0  \n",
      "1    0.0   0.0      0.0    0.0  \n",
      "2    0.0   0.0      0.0    0.0  \n",
      "3    0.0   0.0      0.0    0.0  \n",
      "4    0.0   0.0      0.0    0.0  \n",
      "\n",
      "[5 rows x 756 columns]\n"
     ]
    }
   ],
   "source": [
    "# UPDATE TEXT VECTORIZATION......\n",
    "\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "from spellchecker import SpellChecker\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from spellchecker import SpellChecker\n",
    "\n",
    "# DATA\n",
    "df = pd.read_csv(r\"C:\\Users\\nh013\\Desktop\\chatboot Q and A dataset\\AI.csv\")\n",
    "\n",
    "# DROP ROWS WITH MISSING VALUES\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# REMOVE ANY SPECIAL CHERECTER AND CONVERT LOWERCASE FORM\n",
    "df['Question'] = df['Question'].apply(lambda x: re.sub(r'[^\\w\\s]', '', x.lower()))\n",
    "\n",
    "# REMOVE STOPWORDS\n",
    "stop_words = set(stopwords.words('english'))\n",
    "df['Question'] = df['Question'].apply(lambda x: ' '.join([word for word in x.split() if word not in stop_words]))\n",
    "\n",
    "# LEMMATIZATIONS\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "df['Question'] = df['Question'].apply(lambda x: ' '.join([lemmatizer.lemmatize(word) for word in x.split()]))\n",
    "\n",
    "# SPELL CHECKING\n",
    "spell = SpellChecker()\n",
    "df['Question'] = df['Question'].apply(lambda x: ' '.join([spell.correction(word) if spell.correction(word) is not None else word for word in x.split()]))\n",
    "\n",
    "# ABBREVIATION(Example: 'can't' to 'cannot')\n",
    "abbreviation_mapping = {\n",
    "    \"can't\": \"cannot\",\n",
    "    \"won't\": \"will not\",\n",
    "   \n",
    "}\n",
    "df['Question'] = df['Question'].apply(lambda x: ' '.join([abbreviation_mapping.get(word, word) for word in x.split()]))\n",
    "\n",
    "# REMOVE ANY ROWS WITH MISSING OR EMTY  VALUES AFTER PROCESSING\n",
    "df.dropna(subset=['Question'], inplace=True)\n",
    "\n",
    "# PERFORM TEXT VECTORIZATION\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(df['Question'])\n",
    "\n",
    "# Convert the vectorized data to a DataFrame\n",
    "vectorized_df = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "# CONCATINATE THE VECTORIZED DATA WITH THE ORGINAL DATA FRAME\n",
    "df = pd.concat([df, vectorized_df], axis=1)\n",
    "\n",
    "\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e9f440",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
